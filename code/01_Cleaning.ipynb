{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 1: Standardized Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Sources\n",
    "\n",
    "All origial data sets are included in the [`data`](./data/) directory for this project.\n",
    "\n",
    "* [`act_2017.csv`](./data/act_2017.csv): 2017 ACT Scores by State\n",
    "* [`act_2018.csv`](./data/act_2018.csv): 2018 ACT Scores by State\n",
    "* [`act_2019.csv`](./data/act_2019.csv): 2019 ACT Scores by State\n",
    "* [`sat_2017.csv`](./data/sat_2017.csv): 2017 SAT Scores by State\n",
    "* [`sat_2018.csv`](./data/sat_2018.csv): 2018 SAT Scores by State\n",
    "* [`sat_2019.csv`](./data/sat_2019.csv): 2019 SAT Scores by State\n",
    "* [`sat_2019_by_intended_college_major.csv`](./data/sat_2019_by_intended_college_major.csv): 2019 SAT Scores by Intended College Major\n",
    "* [`sat_act_by_college.csv`](./data/sat_act_by_college.csv): Ranges of Accepted ACT & SAT Student Scores by Colleges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Coding Challenges\n",
    "\n",
    "1. Manually calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code: Calculate the Mean\n",
    "\n",
    "def find_mean(data):\n",
    "    return sum(data)/len(data)\n",
    "\n",
    "find_mean([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Manually calculate standard deviation:\n",
    "\n",
    "    The formula for standard deviation is below:\n",
    "\n",
    "    $$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code: Calculate the Standard Deviation\n",
    "def find_std(data):\n",
    "    sq_differences = [(x - find_mean(data))**2 for x in data]\n",
    "    variance = (sum(sq_differences)) / (len(data) - 1)\n",
    "    return variance**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.224940191045253"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_std([-5, 1, 8, 7, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data cleaning function:\n",
    "    \n",
    "    Write a function that takes in a string that is a number and a percent symbol (ex. '50%', '30.5%', etc.) and converts this to a float that is the decimal approximation of the percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code: Percent string to decimal float\n",
    "def to_decimal(percent):\n",
    "    return (float(percent.split('%')[0]))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.305"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_decimal('30.5%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data cleaning function: lowercase column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return lowercase column names\n",
    "def lowercaseify(df):\n",
    "\tnew_cols = [column.lower().replace(\" \", \"_\") for column in df.columns]\n",
    "\told_cols = df.columns\n",
    "\tto_map = dict(zip(df.columns, new_cols))\n",
    "\tdf.rename(columns = to_map, inplace = True)\n",
    "\n",
    "\tprint(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "# Part 2\n",
    "\n",
    "Part 2 requires knowledge of Pandas, EDA, data cleaning, and data visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import & Cleaning\n",
    "\n",
    "1. Display the data: print the first 5 rows of each dataframe to your Jupyter notebook.\n",
    "2. Check for missing values.\n",
    "3. Check for any obvious issues with the observations (keep in mind the minimum & maximum possible values for each test/subtest).\n",
    "4. Fix any errors you identified in steps 2-3.\n",
    "5. Display the data types of each feature.\n",
    "6. Fix any incorrect data types found in step.\n",
    "7. Rename Columns.\n",
    "8. Drop unnecessary rows (if needed).\n",
    "9. Merge dataframes that can be merged. (pd.concat(axis, row))\n",
    "10. Perform any additional cleaning that you feel is necessary.\n",
    "11. Save your cleaned and merged dataframes as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load ACT data from 2017, 2018, and 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "act_2017 = pd.read_csv('../data/act_2017.csv')\n",
    "act_2018 = pd.read_csv('../data/act_2018.csv')\n",
    "act_2019 = pd.read_csv('../data/act_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT 2017: (52, 7)\n",
      "ACT 2018: (52, 3)\n",
      "ACT 2019: (52, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of data frames\n",
    "print(f'ACT 2017: {act_2017.shape}')\n",
    "print(f'ACT 2018: {act_2018.shape}')\n",
    "print(f'ACT 2019: {act_2019.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merge ACT Data\n",
    "1. Create year columns for each data frame\n",
    "2. Merge on shared columns\n",
    "3. Convert column names to lowercase and reorder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unrelated columns\n",
    "def drop_unrelated(df, related_df):\n",
    "\tfor name in df.columns:\n",
    "\t\tif name not in related_df.columns:\n",
    "\t\t\tdf.drop(columns=name, inplace=True) \n",
    "\t\t\tprint(f\"{name} column dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English column dropped\n",
      "Math column dropped\n",
      "Reading column dropped\n",
      "Science column dropped\n"
     ]
    }
   ],
   "source": [
    "drop_unrelated(act_2017, act_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns with single unique value\n",
    "def single_value_column(df, name, value):\n",
    "    df[name] = value\n",
    "    return df[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      State Participation Composite  year\n",
      "0  National           60%      21.0  2017\n",
      "     State Participation  Composite  year\n",
      "0  Alabama          100%       19.1  2018\n",
      "     State Participation  Composite  year\n",
      "0  Alabama          100%       18.9  2019\n"
     ]
    }
   ],
   "source": [
    "#Create year columns\n",
    "years = [year for year in range(2017,2019+1)]\n",
    "act_dfs = [act_2017, act_2018, act_2019]\n",
    "\n",
    "for i in range(3):\n",
    "    single_value_column(act_dfs[i], 'year', years[i])\n",
    "    print(act_dfs[i].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state', 'participation', 'composite', 'year'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Merge ACT data on shared columns\n",
    "df_act = pd.concat(act_dfs, ignore_index=True)\n",
    "lowercaseify(df_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmed that sum of concatenated rows is 52*3 = 156 and number of shared columns plus the year column is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "df_act = df_act[['year', 'state', 'participation', 'composite']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>participation</th>\n",
       "      <th>composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>National</td>\n",
       "      <td>60%</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>100%</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>65%</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>62%</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>100%</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year     state participation composite\n",
       "0  2017  National           60%      21.0\n",
       "1  2017   Alabama          100%      19.2\n",
       "2  2017    Alaska           65%      19.8\n",
       "3  2017   Arizona           62%      19.7\n",
       "4  2017  Arkansas          100%      19.4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clean ACT data\n",
    "1. Find duplicate and missing values\n",
    "2. Check and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>participation</th>\n",
       "      <th>composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>National</td>\n",
       "      <td>60%</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2019</td>\n",
       "      <td>National</td>\n",
       "      <td>52%</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year     state participation composite\n",
       "0    2017  National           60%      21.0\n",
       "155  2019  National           52%      20.7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act.loc[df_act.state == 'National']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.drop(index=[0,155],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate values in State column\n",
    "len(sorted(df_act['state'].unique()))  #53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace mispelled value in State column\n",
    "df_act['state'].replace('District of columbia', 'District of Columbia', inplace=True)\n",
    "len(sorted(df_act['state'].unique()))  #52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Participation to decimal so datatype is float\n",
    "df_act['participation'] = df_act['participation'].apply(to_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and replace string values in Composite\n",
    "# Change data type to float\n",
    "#df_act['composite'].unique()\n",
    "df_act['composite'] = df_act['composite'].replace('20.2x', 20.2)\n",
    "df_act['composite'] = df_act['composite'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Export Cleaned ACT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.to_csv('../data-clean/act_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load SAT Data for 2017, 2018, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "sat_2017 = pd.read_csv('../data/sat_2017.csv')\n",
    "sat_2018 = pd.read_csv('../data/sat_2018.csv')\n",
    "sat_2019 = pd.read_csv('../data/sat_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAT 2017: (51, 5)\n",
      "SAT 2018: (51, 5)\n",
      "SAT 2019: (53, 5)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of data frames\n",
    "print(f'SAT 2017: {sat_2017.shape}')\n",
    "print(f'SAT 2018: {sat_2018.shape}')\n",
    "print(f'SAT 2019: {sat_2019.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Address 53 row values for 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Puerto Rico', 'Virgin Islands']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_2019 = sat_2019['State'].unique()\n",
    "states_2018 = sat_2018['State'].unique()\n",
    "\n",
    "[i for i in states_2019 if i not in states_2018]  # Puerto Rico, Virgin Islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2019 = sat_2019.loc[(sat_2019['State'] != 'Puerto Rico')]\n",
    "sat_2019 = sat_2019.loc[(sat_2019['State'] != 'Virgin Islands')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_2019.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merge SAT Data\n",
    "1. Create year columns for each data frame\n",
    "2. Merge on shared columns\n",
    "3. Convert column names to lowercase and reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year columns\n",
    "sat_2017['year'] = 2017\n",
    "sat_2018['year'] = 2018\n",
    "sat_2019['year'] = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shared column names\n",
    "sat_2017.rename(columns={'Evidence-Based Reading and Writing': 'EBRW'}, inplace=True)\n",
    "sat_2018.rename(columns={'Evidence-Based Reading and Writing': 'EBRW'},  inplace=True)\n",
    "sat_2019.rename(columns={'Participation Rate': 'Participation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153 entries, 0 to 152\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   State          153 non-null    object\n",
      " 1   Participation  153 non-null    object\n",
      " 2   EBRW           153 non-null    int64 \n",
      " 3   Math           153 non-null    int64 \n",
      " 4   Total          153 non-null    int64 \n",
      " 5   year           153 non-null    int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sat = pd.concat([sat_2017,sat_2018,sat_2019], ignore_index=True)\n",
    "df_sat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['state', 'participation', 'ebrw', 'math', 'total', 'year'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lowercaseify(df_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>participation</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, state, participation, total]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop  \n",
    "df_sat = df_sat[['year', 'state', 'participation', 'total']]\n",
    "df_sat.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clean SAT data\n",
    "1. Find duplicate and missing values\n",
    "2. Check and change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year             0\n",
       "state            0\n",
       "participation    0\n",
       "total            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sat.isna().sum()  # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['participation'] = df_sat['participation'].apply(to_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153 entries, 0 to 152\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   year           153 non-null    int64  \n",
      " 1   state          153 non-null    object \n",
      " 2   participation  153 non-null    float64\n",
      " 3   total          153 non-null    int64  \n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 4.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Export Cleaned SAT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    154.000000\n",
       "mean       0.615260\n",
       "std        0.336501\n",
       "min        0.060000\n",
       "25%        0.290000\n",
       "50%        0.660000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: participation, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act.participation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>participation</th>\n",
       "      <th>composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2019</td>\n",
       "      <td>Maine</td>\n",
       "      <td>0.06</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  state  participation  composite\n",
       "123  2019  Maine           0.06       24.3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act.loc[df_act.participation<=.06] #24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "      <th>participation</th>\n",
       "      <th>composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017</td>\n",
       "      <td>Montana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2017</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2017</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2017</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2017</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2017</td>\n",
       "      <td>Utah</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2017</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2017</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2018</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2018</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2018</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2018</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2018</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2018</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2018</td>\n",
       "      <td>Montana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2018</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2018</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2018</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2018</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2018</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2018</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2018</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2018</td>\n",
       "      <td>Utah</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2018</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2018</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2019</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2019</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2019</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2019</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2019</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2019</td>\n",
       "      <td>Montana</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2019</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2019</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2019</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2019</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2019</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2019</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2019</td>\n",
       "      <td>Utah</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2019</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2019</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year           state  participation  composite\n",
       "1    2017         Alabama            1.0       19.2\n",
       "4    2017        Arkansas            1.0       19.4\n",
       "6    2017        Colorado            1.0       20.8\n",
       "18   2017        Kentucky            1.0       20.0\n",
       "19   2017       Louisiana            1.0       19.5\n",
       "24   2017       Minnesota            1.0       21.5\n",
       "25   2017     Mississippi            1.0       18.6\n",
       "26   2017        Missouri            1.0       20.4\n",
       "27   2017         Montana            1.0       20.3\n",
       "29   2017          Nevada            1.0       17.8\n",
       "34   2017  North Carolina            1.0       19.1\n",
       "37   2017        Oklahoma            1.0       19.4\n",
       "41   2017  South Carolina            1.0       18.7\n",
       "43   2017       Tennessee            1.0       19.8\n",
       "45   2017            Utah            1.0       20.3\n",
       "50   2017       Wisconsin            1.0       20.5\n",
       "51   2017         Wyoming            1.0       20.2\n",
       "52   2018         Alabama            1.0       19.1\n",
       "55   2018        Arkansas            1.0       19.4\n",
       "69   2018        Kentucky            1.0       20.2\n",
       "70   2018       Louisiana            1.0       19.2\n",
       "77   2018     Mississippi            1.0       18.6\n",
       "78   2018        Missouri            1.0       20.0\n",
       "79   2018         Montana            1.0       20.0\n",
       "80   2018        Nebraska            1.0       20.1\n",
       "81   2018          Nevada            1.0       17.7\n",
       "86   2018  North Carolina            1.0       19.1\n",
       "88   2018            Ohio            1.0       20.3\n",
       "89   2018        Oklahoma            1.0       19.3\n",
       "93   2018  South Carolina            1.0       18.3\n",
       "95   2018       Tennessee            1.0       19.6\n",
       "97   2018            Utah            1.0       20.4\n",
       "102  2018       Wisconsin            1.0       20.5\n",
       "103  2018         Wyoming            1.0       20.0\n",
       "104  2019         Alabama            1.0       18.9\n",
       "107  2019        Arkansas            1.0       19.3\n",
       "121  2019        Kentucky            1.0       19.8\n",
       "122  2019       Louisiana            1.0       18.8\n",
       "128  2019     Mississippi            1.0       18.4\n",
       "130  2019         Montana            1.0       19.8\n",
       "131  2019        Nebraska            1.0       20.0\n",
       "132  2019          Nevada            1.0       17.9\n",
       "137  2019  North Carolina            1.0       19.0\n",
       "139  2019            Ohio            1.0       20.0\n",
       "140  2019        Oklahoma            1.0       18.9\n",
       "146  2019       Tennessee            1.0       19.4\n",
       "148  2019            Utah            1.0       20.3\n",
       "153  2019       Wisconsin            1.0       20.3\n",
       "154  2019         Wyoming            1.0       19.8"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_act.loc[df_act.participation>=1] #21.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.to_csv('../data-clean/sat_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load data for SAT and ACT scores by college "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "colleges = pd.read_csv('../data/sat_act_by_college.csv')\n",
    "colleges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clean data from colleges\n",
    "1. Find and replace missing and unwanted values\n",
    "2. Change data types\n",
    "3. Rename and drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and replace zero-width values in Schools\n",
    "#sorted(colleges['School'].unique())\n",
    "def remove_value(series, value):\n",
    "    return [text.replace(value, '') for text in series]\n",
    "\n",
    "colleges['School'] = remove_value(colleges['School'], '\\u200b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Test Optional? values with 1 or 0\n",
    "colleges['Test Optional?'] = colleges['Test Optional?'].apply(lambda x: 1 if x == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty cells\n",
    "colleges.fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More descriptive values for years of test optional policy starting in 2021\n",
    "a = colleges['Applies to Class Year(s)'].unique()\n",
    "b = [1, 2, 0, 4, 3, 5, np.nan]\n",
    "map_years = dict(zip(a, b))\n",
    "\n",
    "colleges['Applies to Class Year(s)'].replace(map_years, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colleges['Policy Details']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`colleges['Policy Details']` contains long strings of text containing info on nuances of their test optional or test required policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Accept Rate to float column\n",
    "colleges['Accept Rate'] = colleges['Accept Rate'].apply(to_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SAT 25th Percentile and 75th Percentile Columns\n",
    "\n",
    "#Remove non-int values\n",
    "colleges['SAT Total 25th-75th Percentile'] = remove_value(colleges['SAT Total 25th-75th Percentile'], '\\u200b\\u200b ')\n",
    "colleges['SAT Total 25th-75th Percentile'].replace('--', np.nan, inplace=True)\n",
    "\n",
    "split_sat_scores = colleges['SAT Total 25th-75th Percentile'].str.split('-',expand=True)\n",
    "colleges['sat_q1'] = split_sat_scores[0].astype('float64')\n",
    "colleges['sat_q3'] = split_sat_scores[1].astype('float64')\n",
    "\n",
    "colleges.drop(columns='SAT Total 25th-75th Percentile', inplace=True)\n",
    "colleges.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ACT 25th Percentile and 75th Percentile Columns\n",
    "\n",
    "#Remove non-int values\n",
    "colleges['ACT Total 25th-75th Percentile'] = remove_value(colleges['ACT Total 25th-75th Percentile'], '\\u200b\\u200b ')\n",
    "colleges['ACT Total 25th-75th Percentile'].replace('--', np.nan, inplace=True)\n",
    "\n",
    "split_act_scores = colleges['ACT Total 25th-75th Percentile'].str.split('-',expand=True)\n",
    "colleges['act_q1'] = split_act_scores[0].astype('float64')\n",
    "colleges['act_q3'] = split_act_scores[1].astype('float64')\n",
    "\n",
    "colleges.drop(columns='ACT Total 25th-75th Percentile', inplace=True)\n",
    "colleges.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercaseify(colleges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges.rename({'applies_to_class_year(s)': 'policy_period'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Export Cleaned Colleges Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges.to_csv('../data-clean/colleges_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
